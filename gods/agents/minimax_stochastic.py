from __future__ import annotations
from typing import Optional
from gods.models import Game_State, Choice
from gods.agents.minimax_search import Search_Context, minimax_search
import copy
import time
import random


class Agent_Minimax_Stochastic:
    def __init__(self, max_depth: int = 5, time_limit: float = 10.0, num_samples: int = 20):
        self.max_depth = max_depth
        self.time_limit = time_limit
        self.num_samples = num_samples
        self.player_index: Optional[int] = None

    def message(self, msg: str):
        pass

    def _sample_state(self, state: Game_State, player_index: int) -> Game_State:
        """Create a sampled state by shuffling hidden information.

        The agent cannot see:
        - Opponent's hand (only knows the count)
        - Opponent's deck order
        - Agent's own deck order
        """
        sampled = copy.deepcopy(state)

        # Shuffle opponent's hidden cards (hand + deck)
        opp_index = 1 - player_index
        opp = sampled.players[opp_index]
        hand_size = len(opp.hand)
        hidden_cards = opp.hand + opp.deck
        random.shuffle(hidden_cards)
        opp.hand = hidden_cards[:hand_size]
        opp.deck = hidden_cards[hand_size:]

        # Shuffle agent's own deck (hand is known, deck order is not)
        me = sampled.players[player_index]
        random.shuffle(me.deck)

        return sampled

    def choose_action(self, state: Game_State, choice: Choice, actions: list) -> int:
        if len(actions) == 0:
            return 0
        if len(actions) == 1:
            return 0

        is_root = self.player_index is None
        if is_root:
            self.player_index = choice.player_index

        selected = self._search(state, choice, actions)

        if is_root:
            self.player_index = None
        print(f"choice: {choice.type}: {actions}")
        print(f"selected: {actions[selected]}")
        return selected

    def _search(self, state: Game_State, choice: Choice, actions: list) -> int:
        """Stochastic minimax with root sampling.

        Runs multiple samples to handle hidden information:
        - Opponent's hand and deck are shuffled together, then redrawn
        - Agent's own deck is shuffled

        For each sample, runs iterative deepening alpha-beta search.
        Returns the action with the highest average score across samples.
        """
        num_actions = len(actions)
        total_scores: list[float] = [0.0] * num_actions
        votes: list[int] = [0] * num_actions
        time_per_sample = self.time_limit / self.num_samples
        overall_start = time.time()

        print(f"started: {choice.type} ({self.num_samples} samples)")

        for _ in range(self.num_samples):
            sampled_state = self._sample_state(state, self.player_index)  # type: ignore[arg-type]

            ctx = Search_Context(
                player_index=self.player_index,  # type: ignore[arg-type]
                start_time=time.time(),
                time_limit=time_per_sample,
            )
            scores = minimax_search(sampled_state, choice, actions, self.max_depth, ctx)
            best_action = max(range(num_actions), key=lambda a: scores[a])
            votes[best_action] += 1


            for i, score in enumerate(scores):
                total_scores[i] += score

        best_action = max(range(num_actions), key=lambda a: votes[a])
        elapsed = time.time() - overall_start
        avg_score = total_scores[best_action] / self.num_samples
        print(
            f"  result: action={actions[best_action]} "
            f"avg_score={avg_score:.2f} time={elapsed:.2f}s"
        )

        return best_action
